#SPDX-License-Identifier: MIT
import logging, os, sys, time, requests, json
import datetime
from multiprocessing import Process, Queue
import pandas as pd
import sqlalchemy as s
from workers.worker_base import Worker
import concurrent.futures as futures

class RepoWorker(Worker):
    def __init__(self, config={}):
        
        # Define the worker's type, which will be used for self identification.
        #   Should be unique among all workers and is the same key used to define 
        #   this worker's settings in the configuration file.
        worker_type = "repo_worker"

        # Define what this worker can be given and know how to interpret
        # given is usually either [['github_url']] or [['git_url']] (depending if your 
        # worker is exclusive to repos that are on the GitHub platform)
        given = [[]]

        # The name the housekeeper/broker use to distinguish the data model this worker can fill
        #   You will also need to name the method that does the collection for this model
        #   in the format *model name*_model() such as fake_data_model() for example
        models = ['repo']

        # Define the tables needed to insert, update, or delete on
        #   The Worker class will set each table you define here as an attribute
        #   so you can reference all of them like self.message_table or self.repo_table
        data_tables = ['repo_groups', 'repo']
        # For most workers you will only need the worker_history and worker_job tables
        #   from the operations schema, these tables are to log worker task histories
        operations_tables = ['worker_history', 'worker_job']

        # Run the general worker initialization
        super().__init__(worker_type, config, given, models, data_tables, operations_tables)

        # Do any additional configuration after the general initialization has been run
        self.config.update(config)

        # If you need to do some preliminary interactions with the database, these MUST go
        # in the model method. The database connection is instantiated only inside of each 
        # data collection process

        # Define data collection info
        self.tool_source = "Repo Worker"
        self.tool_version = "0.0.0"
        self.data_source = "GitHub API"

    def repo_model(self, task):
        """ This method collects repos and repo groups based on the configured criterion
            in augur.config.json.

            :param task: the task generated by the housekeeper and sent to the broker which 
            was then sent to this worker. Takes the example dict format of:
                {
                    'job_type': 'MAINTAIN', 
                    'models': ['repo'], 
                    'display_name': 'repo model for url: https://github.com/vmware/vivace',
                    'given': {}
                }
            :param repo_id: the collect() method queries the repo_id given the git/github url
            and passes it along to make things easier. An int such as: 27869

        """
          
        org_url = f"https://api.github.com/organizations?per_page=100&since={datetime.date.today()}"

        response = requests.get(url=org_url, headers=self.headers)

        try:
            source_orgs = response.json()
        except:
            source_orgs = json.loads(json.dumps(response.text))

        org_action_map = {
            'insert': {
                'source': ['login'],
                'augur': ['rg_name']
            }
        }

        cols_to_query = self.get_relevant_columns(self.repo_groups_table, org_action_map)

        table_values = self.db.execute(s.sql.select(cols_to_query)).fetchall()

        source_orgs_insert, _ = self.organize_needed_data(source_orgs, table_values, 
            list(self.repo_groups_table.primary_key)[0].name, action_map=org_action_map)

        groups_insert = [{
            'rg_name': group['login'],
            'rg_description': group['description'],
            'rg_website': group['url'],
            'tool_source': self.tool_source,
            'tool_version': self.tool_version,
            'data_source': self.data_source
        } for group in source_orgs_insert]

        self.bulk_insert(self.repo_groups_table, insert=groups_insert)

        def retrieve_org_repos(url):
            page_number = 1
            return requests.get(url=url.format(page_number), headers=self.headers)

        def collect_valid_repos():
            with futures.ThreadPoolExecutor(max_workers=10) as executor:
                for org in source_orgs:
                    threads.append(executor.submit(retrieve_org_repos, 
                        org['repos_url'] + "?per_page=40&page={}"))

                gh_merge_fields = ['owner.login']
                augur_merge_fields = ['rg_name']

                for future in futures.as_completed(threads):
                    source_repos = future.result().json()
                    repos_pk = self.enrich_data_primary_keys(source_repos, 
                        self.repo_groups_table, gh_merge_fields, augur_merge_fields)

                    for repo in repos_pk:
                        if int(repo['updated_at'][:4]) < 2020:
                            continue
                        
                        valid_repos.append({
                            'repo_group_id': int(repo['repo_group_id']),
                            'repo_git': repo['url'],
                            'repo_path': repo['full_name'],
                            'repo_name': repo['name'],
                            'repo_type': 'primary',
                            'url': repo['url'],
                            'owner_id': int(repo['id']),
                            'description': repo['description'],
                            'primary_language': repo['language'],
                            'created_at': repo['created_at'],
                            'forked_from': repo['fork'],
                            'updated_at': repo['updated_at'],
                            'tool_source': self.tool_source,
                            'tool_version': self.tool_version,
                            'data_source': self.data_source,
                            'repo_archived': 1 if repo['archived'] else 0,
                            'repo_archived_date_collected': repo['updated_at'] if repo['archived'] else None
                        })


        threads = []
        valid_repos = []
        collect_valid_repos()

        repo_action_map = {
            'insert': {
                'source': ['repo_git'],
                'augur': ['repo_git']
            }
        }

        cols_to_query = self.get_relevant_columns(self.repo_table, repo_action_map)

        table_values = self.db.execute(s.sql.select(cols_to_query)).fetchall()

        repos_insert, _ = self.organize_needed_data(valid_repos, table_values, 
            list(self.repo_table.primary_key)[0].name, action_map=repo_action_map)

        repo_count = 0
        for repo in repos_insert:
            result = self.db.execute(self.repo_table.insert().values(repo))
            self.logger.info("Primary key inserted into the repo table: {}".format(result.inserted_primary_key))
            self.insert_counter += 1
            repo_count += 1
            if repo_count >= 500:
                break

        # self.bulk_insert(self.repo_table, insert=repos_insert)

        # Register this task as completed.
        #   This is a method of the worker class that is required to be called upon completion
        #   of any data collection model, this lets the broker know that this worker is ready
        #   for another task
        self.register_task_completion(task, None, 'repo')

