{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting novel messages based on message reconstruction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from numpy import save, load\n",
    "import sqlalchemy as s\n",
    "import unicodedata\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sqlalchemy import create_engine\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to local Postgres database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = 25827\n",
    "\n",
    "# Fetch PR and issue messages of repo_id\n",
    "join_SQL = s.sql.text(\"\"\"\n",
    "       select message.msg_id, msg_timestamp,  msg_text from augur_data.message\n",
    "left outer join augur_data.pull_request_message_ref on message.msg_id = pull_request_message_ref.msg_id \n",
    "left outer join augur_data.pull_requests on pull_request_message_ref.pull_request_id = pull_requests.pull_request_id\n",
    "where repo_id = :repo_id \n",
    "UNION\n",
    "select message.msg_id, msg_timestamp, msg_text from augur_data.message\n",
    "left outer join augur_data.issue_message_ref on message.msg_id = issue_message_ref.msg_id \n",
    "left outer join augur_data.issues on issue_message_ref.issue_id = issues.issue_id\n",
    "where repo_id = :repo_id\"\"\")\n",
    "\n",
    "df_message = pd.read_sql_query(join_SQL, engine, params={'repo_id': repo_id, 'begin': '2020-03-11 17:21:43'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg_id</th>\n",
       "      <th>msg_timestamp</th>\n",
       "      <th>msg_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1880837</td>\n",
       "      <td>2020-03-11 17:21:43</td>\n",
       "      <td>&gt; Isn't that the point of stabilising an IDE d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2054646</td>\n",
       "      <td>2020-04-06 19:06:06</td>\n",
       "      <td>goodbye friend!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1883945</td>\n",
       "      <td>2020-02-28 18:46:02</td>\n",
       "      <td>&gt; No, at least not intentionally.\\r\\n\\r\\nCould...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1883956</td>\n",
       "      <td>2020-03-10 10:54:39</td>\n",
       "      <td>@mauromol Thanks a lot for trying the latest C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1933586</td>\n",
       "      <td>2018-12-05 06:07:18</td>\n",
       "      <td>Doesn't seem to be necessary. Extension pack l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>1859406</td>\n",
       "      <td>2020-02-25 11:00:08</td>\n",
       "      <td>Hey @sandorApati, hard to say remotely what ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>1933712</td>\n",
       "      <td>2018-10-09 15:25:58</td>\n",
       "      <td>&gt; I can confirm that the Spring-Data JPA Conte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>1942947</td>\n",
       "      <td>2020-04-06 19:03:29</td>\n",
       "      <td>![image](https://user-images.githubusercontent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>1922858</td>\n",
       "      <td>2020-04-01 16:27:00</td>\n",
       "      <td>Closing because of no response from @reszy wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>1887748</td>\n",
       "      <td>2020-03-09 16:25:37</td>\n",
       "      <td>It'd be great to look at this XML file... What...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1866 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       msg_id       msg_timestamp  \\\n",
       "0     1880837 2020-03-11 17:21:43   \n",
       "1     2054646 2020-04-06 19:06:06   \n",
       "2     1883945 2020-02-28 18:46:02   \n",
       "3     1883956 2020-03-10 10:54:39   \n",
       "4     1933586 2018-12-05 06:07:18   \n",
       "...       ...                 ...   \n",
       "1861  1859406 2020-02-25 11:00:08   \n",
       "1862  1933712 2018-10-09 15:25:58   \n",
       "1863  1942947 2020-04-06 19:03:29   \n",
       "1864  1922858 2020-04-01 16:27:00   \n",
       "1865  1887748 2020-03-09 16:25:37   \n",
       "\n",
       "                                               msg_text  \n",
       "0     > Isn't that the point of stabilising an IDE d...  \n",
       "1                                       goodbye friend!  \n",
       "2     > No, at least not intentionally.\\r\\n\\r\\nCould...  \n",
       "3     @mauromol Thanks a lot for trying the latest C...  \n",
       "4     Doesn't seem to be necessary. Extension pack l...  \n",
       "...                                                 ...  \n",
       "1861  Hey @sandorApati, hard to say remotely what ex...  \n",
       "1862  > I can confirm that the Spring-Data JPA Conte...  \n",
       "1863  ![image](https://user-images.githubusercontent...  \n",
       "1864  Closing because of no response from @reszy wit...  \n",
       "1865  It'd be great to look at this XML file... What...  \n",
       "\n",
       "[1866 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_message.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing text\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "punc=list(string.punctuation)\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "snowBallStemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Expanding contractions\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "# Removing stop words, punctuations, spaces, stemming...\n",
    "def clean_text(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    for tag in soup.find_all('strong'):\n",
    "        tag.replaceWith('')\n",
    "        text = soup.get_text()\n",
    "\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\r', ' ', text)\n",
    "    text = re.sub('[()){}]', ' ', text)\n",
    "    text = re.sub('\\<[^<>]*\\>', '', text)\n",
    "    text = re.sub('\\`[^``]*\\`', '', text)\n",
    "    \n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "    # Removes personal mentions like @abc, and email addresses\n",
    "    s = re.sub(r'\\w*@\\w*', ' ', text)   \n",
    "\n",
    "    # Removes attached links\n",
    "    s=' '.join(word for word in s.split(' ') if not word.startswith('http'))   \n",
    "\n",
    "    # Separates joint words\n",
    "    # Removes punctuation\n",
    "    s = re.sub('[\\.\\-\\_\\\\/&]', ' ', s)\n",
    "    s = \"\".join([word.lower() for word in s if word not in punc])\n",
    "    s = word_tokenize(s)\n",
    "\n",
    "    # Stemming\n",
    "    s = \" \".join([snowBallStemmer.stem(word) for word in s if len(word)<=30 and word not in stopword])              \n",
    "\n",
    "    # Tokenization\n",
    "    s = re.sub('[0-9]+', '', s)\n",
    "    s = re.sub('lgtm', 'look good', s)\n",
    "    return s\n",
    "\n",
    "# Normalize corpus\n",
    "def normalize_corpus(text,contraction_expansion=True,clean=True):\n",
    "    if contraction_expansion:\n",
    "        text = expand_contractions(text)\n",
    "    if clean:\n",
    "        text = clean_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_message['cleaned_msg_text'] = df_message['msg_text'].map(lambda x: normalize_corpus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg_id</th>\n",
       "      <th>msg_timestamp</th>\n",
       "      <th>msg_text</th>\n",
       "      <th>cleaned_msg_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1880837</td>\n",
       "      <td>2020-03-11 17:21:43</td>\n",
       "      <td>&gt; Isn't that the point of stabilising an IDE d...</td>\n",
       "      <td>point stabilis ide despit usag extern depend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2054646</td>\n",
       "      <td>2020-04-06 19:06:06</td>\n",
       "      <td>goodbye friend!</td>\n",
       "      <td>goodby friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1883945</td>\n",
       "      <td>2020-02-28 18:46:02</td>\n",
       "      <td>&gt; No, at least not intentionally.\\r\\n\\r\\nCould...</td>\n",
       "      <td>least intent could batch event work areadi don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1883956</td>\n",
       "      <td>2020-03-10 10:54:39</td>\n",
       "      <td>@mauromol Thanks a lot for trying the latest C...</td>\n",
       "      <td>thank lot tri latest ci build attach log great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1933586</td>\n",
       "      <td>2018-12-05 06:07:18</td>\n",
       "      <td>Doesn't seem to be necessary. Extension pack l...</td>\n",
       "      <td>seem necessari extens pack list extens without...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>1859406</td>\n",
       "      <td>2020-02-25 11:00:08</td>\n",
       "      <td>Hey @sandorApati, hard to say remotely what ex...</td>\n",
       "      <td>hey hard say remot exact go wrong doubl check ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>1933712</td>\n",
       "      <td>2018-10-09 15:25:58</td>\n",
       "      <td>&gt; I can confirm that the Spring-Data JPA Conte...</td>\n",
       "      <td>confirm spring data jpa content assist work an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>1942947</td>\n",
       "      <td>2020-04-06 19:03:29</td>\n",
       "      <td>![image](https://user-images.githubusercontent...</td>\n",
       "      <td>imag mean understand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>1922858</td>\n",
       "      <td>2020-04-01 16:27:00</td>\n",
       "      <td>Closing because of no response from @reszy wit...</td>\n",
       "      <td>close respons info tri sort thing recent seem ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>1887748</td>\n",
       "      <td>2020-03-09 16:25:37</td>\n",
       "      <td>It'd be great to look at this XML file... What...</td>\n",
       "      <td>would great look xml file kind ca way insid at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1866 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       msg_id       msg_timestamp  \\\n",
       "0     1880837 2020-03-11 17:21:43   \n",
       "1     2054646 2020-04-06 19:06:06   \n",
       "2     1883945 2020-02-28 18:46:02   \n",
       "3     1883956 2020-03-10 10:54:39   \n",
       "4     1933586 2018-12-05 06:07:18   \n",
       "...       ...                 ...   \n",
       "1861  1859406 2020-02-25 11:00:08   \n",
       "1862  1933712 2018-10-09 15:25:58   \n",
       "1863  1942947 2020-04-06 19:03:29   \n",
       "1864  1922858 2020-04-01 16:27:00   \n",
       "1865  1887748 2020-03-09 16:25:37   \n",
       "\n",
       "                                               msg_text  \\\n",
       "0     > Isn't that the point of stabilising an IDE d...   \n",
       "1                                       goodbye friend!   \n",
       "2     > No, at least not intentionally.\\r\\n\\r\\nCould...   \n",
       "3     @mauromol Thanks a lot for trying the latest C...   \n",
       "4     Doesn't seem to be necessary. Extension pack l...   \n",
       "...                                                 ...   \n",
       "1861  Hey @sandorApati, hard to say remotely what ex...   \n",
       "1862  > I can confirm that the Spring-Data JPA Conte...   \n",
       "1863  ![image](https://user-images.githubusercontent...   \n",
       "1864  Closing because of no response from @reszy wit...   \n",
       "1865  It'd be great to look at this XML file... What...   \n",
       "\n",
       "                                       cleaned_msg_text  \n",
       "0     point stabilis ide despit usag extern depend l...  \n",
       "1                                         goodby friend  \n",
       "2     least intent could batch event work areadi don...  \n",
       "3     thank lot tri latest ci build attach log great...  \n",
       "4     seem necessari extens pack list extens without...  \n",
       "...                                                 ...  \n",
       "1861  hey hard say remot exact go wrong doubl check ...  \n",
       "1862  confirm spring data jpa content assist work an...  \n",
       "1863                               imag mean understand  \n",
       "1864  close respons info tri sort thing recent seem ...  \n",
       "1865  would great look xml file kind ca way insid at...  \n",
       "\n",
       "[1866 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Doc2Vec from custom dataset messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_excel('./mod_train.xlsx', names=['Text','score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.drop(['score'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://screenhero.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://msdn.microsoft.com/en-us/library/ms678505(VS.85).aspx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://mathiasbynens.be/notes/reserved-keywords\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://en.wikipedia.beta.wmflabs.org/wiki/Hurricane_Celeste_(1972)\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://cwiki.apache.org/confluence/display/OFBADMIN/OFBiz+Contributors+Best+Practices#OFBizContributorsBestPractices-HowtoSendinYourContributions%28orhowtocreateandapplypatches%29\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.w3.org/People/Bos/CSS-variables\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://stackoverflow.com/questions/576792/asp-mvc-server-requirements-is-server-2008-very-desirable\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.freevbcode.com/ShowCode.asp?ID=3723\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://stackoverflow.com/questions/3167909/what-motivates-you-to-document-not-document-your-code\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"http://blog.jerodsanto.net/2009/05/jquery-set-mouse-focus-on-page-load/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://gerrit-review.googlesource.com/Documentation/dev-contributing.html#_tests\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    }
   ],
   "source": [
    "df_all['cleaned_msg_text'] = df_all['Text'].map(lambda x: normalize_corpus(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split past present data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_message['msg_timestamp'] = [d.date() for d in df_message['msg_timestamp']]\n",
    "df_message = df_message.sort_values(by='msg_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past = df_message[df_message['msg_timestamp'].astype(str)< '2020-04-01']\n",
    "df_present = df_message[df_message['msg_timestamp'].astype(str)>= '2020-04-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(566, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_present.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_past.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making word embeddings using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer which converts text corpus into document vectors.\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils as skl_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "def build_model(max_epochs, vec_size, alpha, tag_data):    \n",
    "    model = Doc2Vec(vector_size=vec_size, alpha=alpha,min_alpha=0.00025, min_count=2, dm=1)\n",
    "    model.build_vocab(tag_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train(skl_utils.shuffle(tag_data),\n",
    "                   total_examples=model.corpus_count,\n",
    "                   epochs=model.epochs)\n",
    "\n",
    "        model.alpha -= 0.0002\n",
    "\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    model.save(\"doc2vec.model\")\n",
    "    print(\"Model Saved\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelqq = Doc2Vec.load(\"doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "df_x = pd.DataFrame(df_all['cleaned_msg_text'])\n",
    "tag_data = [TaggedDocument(str(row['cleaned_msg_text']).split(), [index]) for index, row in df_x.iterrows()]\n",
    "# print(tag_data)\n",
    "model = build_model(max_epochs=100, vec_size=300, alpha=0.01, tag_data=tag_data)\n",
    "doc2vec_vectors = np.array([model.infer_vector(str(row['cleaned_msg_text']).split())for index, row in df_past.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scream', 0.48170191049575806),\n",
       " ('ugh', 0.4529797434806824),\n",
       " ('typedef', 0.44843924045562744),\n",
       " ('terribl', 0.44335469603538513),\n",
       " ('commitupdatecommand', 0.4413297772407532),\n",
       " ('readabl', 0.44015777111053467),\n",
       " ('unsaf', 0.4396229684352875),\n",
       " ('blooper', 0.435542494058609),\n",
       " ('indexof', 0.4317985773086548),\n",
       " ('slerp', 0.43164515495300293)]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def autoencoder(vec_input, train):\n",
    "\n",
    "    input_dim = Input(shape = (vec_input, ))\n",
    "    encoded1 = Dense(vec_input//2, activation = 'sigmoid')(input_dim)\n",
    "    encoded2 = Dense(1, activation = 'sigmoid')(encoded1)\n",
    "\n",
    "    # Decoder Layers\n",
    "    decoded1 = Dense(vec_input//2, activation = 'tanh')(encoded2)\n",
    "    decoded2 = Dense(vec_input, activation = 'tanh')(decoded1)\n",
    "\n",
    "    # Combine Encoder and Deocder layers\n",
    "    model = Model(inputs = input_dim, outputs = decoded2)\n",
    "\n",
    "    # Compile the Model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    model.summary()\n",
    "    model.fit(train, train, epochs = 50)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 151       \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 150)               300       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 90,901\n",
      "Trainable params: 90,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1300/1300 [==============================] - 0s 111us/step - loss: 4.2057e-04 - mean_squared_error: 4.2057e-04\n",
      "Epoch 2/50\n",
      "1300/1300 [==============================] - 0s 78us/step - loss: 1.6567e-04 - mean_squared_error: 1.6567e-04\n",
      "Epoch 3/50\n",
      "1300/1300 [==============================] - 0s 54us/step - loss: 1.6545e-04 - mean_squared_error: 1.6545e-04\n",
      "Epoch 4/50\n",
      "1300/1300 [==============================] - 0s 72us/step - loss: 1.6381e-04 - mean_squared_error: 1.6381e-04\n",
      "Epoch 5/50\n",
      "1300/1300 [==============================] - 0s 68us/step - loss: 1.6081e-04 - mean_squared_error: 1.6081e-04\n",
      "Epoch 6/50\n",
      "1300/1300 [==============================] - 0s 77us/step - loss: 1.4397e-04 - mean_squared_error: 1.4397e-04\n",
      "Epoch 7/50\n",
      "1300/1300 [==============================] - 0s 69us/step - loss: 8.0225e-05 - mean_squared_error: 8.0225e-05\n",
      "Epoch 8/50\n",
      "1300/1300 [==============================] - 0s 71us/step - loss: 3.7877e-05 - mean_squared_error: 3.7877e-05\n",
      "Epoch 9/50\n",
      "1300/1300 [==============================] - 0s 130us/step - loss: 3.2113e-05 - mean_squared_error: 3.2113e-05\n",
      "Epoch 10/50\n",
      "1300/1300 [==============================] - 0s 131us/step - loss: 3.1729e-05 - mean_squared_error: 3.1729e-05\n",
      "Epoch 11/50\n",
      "1300/1300 [==============================] - 0s 79us/step - loss: 3.1435e-05 - mean_squared_error: 3.1435e-05\n",
      "Epoch 12/50\n",
      "1300/1300 [==============================] - 0s 80us/step - loss: 3.1451e-05 - mean_squared_error: 3.1451e-05\n",
      "Epoch 13/50\n",
      "1300/1300 [==============================] - 0s 78us/step - loss: 3.1334e-05 - mean_squared_error: 3.1334e-05\n",
      "Epoch 14/50\n",
      "1300/1300 [==============================] - 0s 72us/step - loss: 3.1322e-05 - mean_squared_error: 3.1322e-05\n",
      "Epoch 15/50\n",
      "1300/1300 [==============================] - 0s 92us/step - loss: 3.1554e-05 - mean_squared_error: 3.1554e-05\n",
      "Epoch 16/50\n",
      "1300/1300 [==============================] - 0s 87us/step - loss: 3.1369e-05 - mean_squared_error: 3.1369e-05\n",
      "Epoch 17/50\n",
      "1300/1300 [==============================] - 0s 70us/step - loss: 3.1271e-05 - mean_squared_error: 3.1271e-05\n",
      "Epoch 18/50\n",
      "1300/1300 [==============================] - 0s 78us/step - loss: 3.1357e-05 - mean_squared_error: 3.1357e-05\n",
      "Epoch 19/50\n",
      "1300/1300 [==============================] - 0s 72us/step - loss: 3.1324e-05 - mean_squared_error: 3.1324e-05\n",
      "Epoch 20/50\n",
      "1300/1300 [==============================] - 0s 74us/step - loss: 3.1303e-05 - mean_squared_error: 3.1303e-05\n",
      "Epoch 21/50\n",
      "1300/1300 [==============================] - 0s 73us/step - loss: 3.1385e-05 - mean_squared_error: 3.1385e-05\n",
      "Epoch 22/50\n",
      "1300/1300 [==============================] - 0s 84us/step - loss: 3.1286e-05 - mean_squared_error: 3.1286e-05\n",
      "Epoch 23/50\n",
      "1300/1300 [==============================] - 0s 123us/step - loss: 3.1361e-05 - mean_squared_error: 3.1361e-05\n",
      "Epoch 24/50\n",
      "1300/1300 [==============================] - 0s 123us/step - loss: 3.1329e-05 - mean_squared_error: 3.1329e-05\n",
      "Epoch 25/50\n",
      "1300/1300 [==============================] - 0s 77us/step - loss: 3.1435e-05 - mean_squared_error: 3.1435e-05\n",
      "Epoch 26/50\n",
      "1300/1300 [==============================] - 0s 79us/step - loss: 3.1350e-05 - mean_squared_error: 3.1350e-05\n",
      "Epoch 27/50\n",
      "1300/1300 [==============================] - 0s 73us/step - loss: 3.1429e-05 - mean_squared_error: 3.1429e-05\n",
      "Epoch 28/50\n",
      "1300/1300 [==============================] - 0s 100us/step - loss: 3.1366e-05 - mean_squared_error: 3.1366e-05\n",
      "Epoch 29/50\n",
      "1300/1300 [==============================] - 0s 76us/step - loss: 3.1347e-05 - mean_squared_error: 3.1347e-05\n",
      "Epoch 30/50\n",
      "1300/1300 [==============================] - 0s 72us/step - loss: 3.1359e-05 - mean_squared_error: 3.1359e-05\n",
      "Epoch 31/50\n",
      "1300/1300 [==============================] - 0s 73us/step - loss: 3.1359e-05 - mean_squared_error: 3.1359e-05\n",
      "Epoch 32/50\n",
      "1300/1300 [==============================] - 0s 71us/step - loss: 3.1471e-05 - mean_squared_error: 3.1471e-05\n",
      "Epoch 33/50\n",
      "1300/1300 [==============================] - 0s 79us/step - loss: 3.1440e-05 - mean_squared_error: 3.1440e-05\n",
      "Epoch 34/50\n",
      "1300/1300 [==============================] - 0s 90us/step - loss: 3.1297e-05 - mean_squared_error: 3.1297e-05\n",
      "Epoch 35/50\n",
      "1300/1300 [==============================] - 0s 124us/step - loss: 3.1349e-05 - mean_squared_error: 3.1349e-05\n",
      "Epoch 36/50\n",
      "1300/1300 [==============================] - 0s 115us/step - loss: 3.1433e-05 - mean_squared_error: 3.1433e-05\n",
      "Epoch 37/50\n",
      "1300/1300 [==============================] - 0s 75us/step - loss: 3.1447e-05 - mean_squared_error: 3.1447e-05\n",
      "Epoch 38/50\n",
      "1300/1300 [==============================] - 0s 77us/step - loss: 3.1286e-05 - mean_squared_error: 3.1286e-05\n",
      "Epoch 39/50\n",
      "1300/1300 [==============================] - 0s 90us/step - loss: 3.1808e-05 - mean_squared_error: 3.1808e-05\n",
      "Epoch 40/50\n",
      "1300/1300 [==============================] - 0s 70us/step - loss: 3.1438e-05 - mean_squared_error: 3.1438e-05\n",
      "Epoch 41/50\n",
      "1300/1300 [==============================] - 0s 76us/step - loss: 3.1559e-05 - mean_squared_error: 3.1559e-05\n",
      "Epoch 42/50\n",
      "1300/1300 [==============================] - 0s 72us/step - loss: 3.1386e-05 - mean_squared_error: 3.1386e-05\n",
      "Epoch 43/50\n",
      "1300/1300 [==============================] - 0s 72us/step - loss: 3.1285e-05 - mean_squared_error: 3.1285e-05\n",
      "Epoch 44/50\n",
      "1300/1300 [==============================] - 0s 78us/step - loss: 3.1350e-05 - mean_squared_error: 3.1350e-05\n",
      "Epoch 45/50\n",
      "1300/1300 [==============================] - 0s 74us/step - loss: 3.1361e-05 - mean_squared_error: 3.1361e-05\n",
      "Epoch 46/50\n",
      "1300/1300 [==============================] - 0s 64us/step - loss: 3.1390e-05 - mean_squared_error: 3.1390e-05\n",
      "Epoch 47/50\n",
      "1300/1300 [==============================] - 0s 74us/step - loss: 3.1466e-05 - mean_squared_error: 3.1466e-05\n",
      "Epoch 48/50\n",
      "1300/1300 [==============================] - 0s 62us/step - loss: 3.1393e-05 - mean_squared_error: 3.1393e-05\n",
      "Epoch 49/50\n",
      "1300/1300 [==============================] - 0s 75us/step - loss: 3.1391e-05 - mean_squared_error: 3.1391e-05\n",
      "Epoch 50/50\n",
      "1300/1300 [==============================] - 0s 73us/step - loss: 3.1535e-05 - mean_squared_error: 3.1535e-05\n"
     ]
    }
   ],
   "source": [
    "ae1 = autoencoder(300, doc2vec_vectors)\n",
    "pred_train = ae1.predict(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate reconstruction error & otsu threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction(pred, val):\n",
    "    rec_error = []\n",
    "    for i in range(len(pred)):\n",
    "        rec_error.append(np.linalg.norm(pred[i] - val[i]))\n",
    "    rec_error = np.array(rec_error)\n",
    "    return rec_error\n",
    "\n",
    "def thresholding(rec_error, val):\n",
    "    threshold = threshold_otsu(rec_error)\n",
    "    normals = []\n",
    "    for i in range(len(rec_error)):\n",
    "        if rec_error[i] < threshold:\n",
    "            normals.append(val[i])\n",
    "    normals = np.array(normals)\n",
    "    return threshold, normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify normal data from 1st AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_error1 = reconstruction(pred_train, doc2vec_vectors)\n",
    "threshold1, normal_data = thresholding(rec_error1, doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1054"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['25827_uniq.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Save to file in the current working directory\n",
    "joblib_file = \"{r_id}_uniq.pkl\".format(r_id = repo_id)\n",
    "joblib.dump(auto_encoder, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting on present data\n",
    "doc2vec_vectors_test = np.array([model.infer_vector(str(row['cleaned_msg_text']).split())for index, row in df_present.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 2nd AE with normal data alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1)                 151       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 150)               300       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 300)               45300     \n",
      "=================================================================\n",
      "Total params: 90,901\n",
      "Trainable params: 90,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1054/1054 [==============================] - 0s 125us/step - loss: 2.8094e-04 - mean_squared_error: 2.8094e-04\n",
      "Epoch 2/50\n",
      "1054/1054 [==============================] - 0s 55us/step - loss: 1.4526e-05 - mean_squared_error: 1.4526e-05\n",
      "Epoch 3/50\n",
      "1054/1054 [==============================] - 0s 88us/step - loss: 1.0327e-05 - mean_squared_error: 1.0327e-05\n",
      "Epoch 4/50\n",
      "1054/1054 [==============================] - 0s 63us/step - loss: 1.0178e-05 - mean_squared_error: 1.0178e-05\n",
      "Epoch 5/50\n",
      "1054/1054 [==============================] - 0s 73us/step - loss: 1.0179e-05 - mean_squared_error: 1.0179e-05\n",
      "Epoch 6/50\n",
      "1054/1054 [==============================] - 0s 80us/step - loss: 1.0186e-05 - mean_squared_error: 1.0186e-05\n",
      "Epoch 7/50\n",
      "1054/1054 [==============================] - 0s 80us/step - loss: 1.0243e-05 - mean_squared_error: 1.0243e-05\n",
      "Epoch 8/50\n",
      "1054/1054 [==============================] - 0s 68us/step - loss: 1.0240e-05 - mean_squared_error: 1.0240e-05\n",
      "Epoch 9/50\n",
      "1054/1054 [==============================] - 0s 77us/step - loss: 1.0247e-05 - mean_squared_error: 1.0247e-05\n",
      "Epoch 10/50\n",
      "1054/1054 [==============================] - 0s 68us/step - loss: 1.0276e-05 - mean_squared_error: 1.0276e-05\n",
      "Epoch 11/50\n",
      "1054/1054 [==============================] - 0s 79us/step - loss: 1.0290e-05 - mean_squared_error: 1.0290e-05\n",
      "Epoch 12/50\n",
      "1054/1054 [==============================] - 0s 92us/step - loss: 1.0268e-05 - mean_squared_error: 1.0268e-05\n",
      "Epoch 13/50\n",
      "1054/1054 [==============================] - 0s 78us/step - loss: 1.0255e-05 - mean_squared_error: 1.0255e-05\n",
      "Epoch 14/50\n",
      "1054/1054 [==============================] - 0s 80us/step - loss: 1.0244e-05 - mean_squared_error: 1.0244e-05\n",
      "Epoch 15/50\n",
      "1054/1054 [==============================] - 0s 73us/step - loss: 1.0247e-05 - mean_squared_error: 1.0247e-05\n",
      "Epoch 16/50\n",
      "1054/1054 [==============================] - 0s 81us/step - loss: 1.0281e-05 - mean_squared_error: 1.0281e-05\n",
      "Epoch 17/50\n",
      "1054/1054 [==============================] - 0s 72us/step - loss: 1.0319e-05 - mean_squared_error: 1.0319e-05\n",
      "Epoch 18/50\n",
      "1054/1054 [==============================] - 0s 71us/step - loss: 1.0244e-05 - mean_squared_error: 1.0244e-05\n",
      "Epoch 19/50\n",
      "1054/1054 [==============================] - 0s 78us/step - loss: 1.0281e-05 - mean_squared_error: 1.0281e-05\n",
      "Epoch 20/50\n",
      "1054/1054 [==============================] - 0s 84us/step - loss: 1.0323e-05 - mean_squared_error: 1.0323e-05\n",
      "Epoch 21/50\n",
      "1054/1054 [==============================] - 0s 69us/step - loss: 1.0239e-05 - mean_squared_error: 1.0239e-05\n",
      "Epoch 22/50\n",
      "1054/1054 [==============================] - 0s 80us/step - loss: 1.0246e-05 - mean_squared_error: 1.0246e-05\n",
      "Epoch 23/50\n",
      "1054/1054 [==============================] - 0s 74us/step - loss: 1.0237e-05 - mean_squared_error: 1.0237e-05\n",
      "Epoch 24/50\n",
      "1054/1054 [==============================] - 0s 71us/step - loss: 1.0321e-05 - mean_squared_error: 1.0321e-05\n",
      "Epoch 25/50\n",
      "1054/1054 [==============================] - 0s 82us/step - loss: 1.0271e-05 - mean_squared_error: 1.0271e-05\n",
      "Epoch 26/50\n",
      "1054/1054 [==============================] - 0s 73us/step - loss: 1.0223e-05 - mean_squared_error: 1.0223e-05\n",
      "Epoch 27/50\n",
      "1054/1054 [==============================] - 0s 82us/step - loss: 1.0222e-05 - mean_squared_error: 1.0222e-05\n",
      "Epoch 28/50\n",
      "1054/1054 [==============================] - 0s 85us/step - loss: 1.0307e-05 - mean_squared_error: 1.0307e-05\n",
      "Epoch 29/50\n",
      "1054/1054 [==============================] - 0s 78us/step - loss: 1.0282e-05 - mean_squared_error: 1.0282e-05\n",
      "Epoch 30/50\n",
      "1054/1054 [==============================] - 0s 81us/step - loss: 1.0316e-05 - mean_squared_error: 1.0316e-05\n",
      "Epoch 31/50\n",
      "1054/1054 [==============================] - 0s 80us/step - loss: 1.0300e-05 - mean_squared_error: 1.0300e-05\n",
      "Epoch 32/50\n",
      "1054/1054 [==============================] - 0s 84us/step - loss: 1.0173e-05 - mean_squared_error: 1.0173e-05\n",
      "Epoch 33/50\n",
      "1054/1054 [==============================] - 0s 82us/step - loss: 1.0325e-05 - mean_squared_error: 1.0325e-05\n",
      "Epoch 34/50\n",
      "1054/1054 [==============================] - 0s 82us/step - loss: 1.0226e-05 - mean_squared_error: 1.0226e-05\n",
      "Epoch 35/50\n",
      "1054/1054 [==============================] - 0s 71us/step - loss: 1.0229e-05 - mean_squared_error: 1.0229e-05\n",
      "Epoch 36/50\n",
      "1054/1054 [==============================] - 0s 84us/step - loss: 1.0174e-05 - mean_squared_error: 1.0174e-05\n",
      "Epoch 37/50\n",
      "1054/1054 [==============================] - 0s 71us/step - loss: 1.0231e-05 - mean_squared_error: 1.0231e-05\n",
      "Epoch 38/50\n",
      "1054/1054 [==============================] - 0s 75us/step - loss: 1.0063e-05 - mean_squared_error: 1.0063e-05\n",
      "Epoch 39/50\n",
      "1054/1054 [==============================] - 0s 80us/step - loss: 9.9447e-06 - mean_squared_error: 9.9447e-06\n",
      "Epoch 40/50\n",
      "1054/1054 [==============================] - 0s 79us/step - loss: 9.8306e-06 - mean_squared_error: 9.8306e-06\n",
      "Epoch 41/50\n",
      "1054/1054 [==============================] - 0s 56us/step - loss: 9.8223e-06 - mean_squared_error: 9.8223e-06\n",
      "Epoch 42/50\n",
      "1054/1054 [==============================] - 0s 55us/step - loss: 9.7023e-06 - mean_squared_error: 9.7023e-06\n",
      "Epoch 43/50\n",
      "1054/1054 [==============================] - 0s 82us/step - loss: 9.4481e-06 - mean_squared_error: 9.4481e-06\n",
      "Epoch 44/50\n",
      "1054/1054 [==============================] - 0s 103us/step - loss: 9.2783e-06 - mean_squared_error: 9.2783e-06\n",
      "Epoch 45/50\n",
      "1054/1054 [==============================] - 0s 90us/step - loss: 9.0580e-06 - mean_squared_error: 9.0580e-06\n",
      "Epoch 46/50\n",
      "1054/1054 [==============================] - 0s 83us/step - loss: 8.8510e-06 - mean_squared_error: 8.8510e-06\n",
      "Epoch 47/50\n",
      "1054/1054 [==============================] - 0s 88us/step - loss: 8.4604e-06 - mean_squared_error: 8.4604e-06\n",
      "Epoch 48/50\n",
      "1054/1054 [==============================] - 0s 79us/step - loss: 8.0554e-06 - mean_squared_error: 8.0554e-06\n",
      "Epoch 49/50\n",
      "1054/1054 [==============================] - 0s 81us/step - loss: 7.6227e-06 - mean_squared_error: 7.6227e-06\n",
      "Epoch 50/50\n",
      "1054/1054 [==============================] - 0s 123us/step - loss: 7.2396e-06 - mean_squared_error: 7.2396e-06\n"
     ]
    }
   ],
   "source": [
    "ae2 = autoencoder(300, normal_data)\n",
    "predicted_vectors1 = ae2.predict(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_error2 = reconstruction(predicted_vectors1, doc2vec_vectors)\n",
    "threshold2, normal_data = thresholding(rec_error2, doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20612182"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate novel count on present data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_vectors_test = ae2.predict(doc2vec_vectors_test)\n",
    "rec_error_test = reconstruction(predicted_vectors_test, doc2vec_vectors_test)\n",
    "c=0\n",
    "for i in range(len(rec_error_test)):\n",
    "    if rec_error_test[i] > threshold2:\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the cosine similarity between predicted and Doc2Vec vectors, to get reconstruction errors\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def key_cosine_similarity(tupple):\n",
    "    return tupple[1]\n",
    "\n",
    "def get_computed_similarities(vectors, predicted_vectors, reverse=False):\n",
    "    data_size = len(df_present)\n",
    "    cosine_similarities = []\n",
    "    cosine_sim_values = []\n",
    "    for i in range(data_size):\n",
    "        cosine_sim_val = (1 - cosine(vectors[i], predicted_vectors[i]))\n",
    "        cosine_similarities.append((df_present['msg_id'].iloc[i], cosine_sim_val))\n",
    "        cosine_sim_values.append(cosine_sim_val)\n",
    "    \n",
    "    df_present['uniqueness_score'] = cosine_sim_values\n",
    "    return sorted(cosine_similarities, key=key_cosine_similarity,reverse=reverse)\n",
    "\n",
    "def display_unique(sorted_cosine_similarities):\n",
    "    i=0\n",
    "    unique_message_list=[]\n",
    "    cos_val = []\n",
    "    index, cosine_sim_val = sorted_cosine_similarities[0]\n",
    "    while cosine_sim_val<=-0.1:\n",
    "        if cosine_sim_val not in cos_val:\n",
    "            unique_message_list.append(index)\n",
    "            cos_val.append(cosine_sim_val)\n",
    "            print('Message id: ', index)  \n",
    "            print('Cosine Sim Val :', cosine_sim_val)\n",
    "        i+=1    \n",
    "        index, cosine_sim_val = sorted_cosine_similarities[i]\n",
    "        \n",
    "    return unique_message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message id:  2012691\n",
      "Cosine Sim Val : -0.2495095133781433\n",
      "Message id:  2047053\n",
      "Cosine Sim Val : -0.22886519134044647\n",
      "Message id:  2032146\n",
      "Cosine Sim Val : -0.21972420811653137\n",
      "Message id:  2051407\n",
      "Cosine Sim Val : -0.21733559668064117\n",
      "Message id:  2054638\n",
      "Cosine Sim Val : -0.1771014779806137\n",
      "Message id:  1920180\n",
      "Cosine Sim Val : -0.17360268533229828\n",
      "Message id:  2085361\n",
      "Cosine Sim Val : -0.16967616975307465\n",
      "Message id:  2012725\n",
      "Cosine Sim Val : -0.1646515429019928\n",
      "Message id:  1933596\n",
      "Cosine Sim Val : -0.14073914289474487\n",
      "Message id:  1942982\n",
      "Cosine Sim Val : -0.137152761220932\n",
      "Message id:  2069007\n",
      "Cosine Sim Val : -0.13674315810203552\n",
      "Message id:  2059090\n",
      "Cosine Sim Val : -0.13608397543430328\n",
      "Message id:  2078076\n",
      "Cosine Sim Val : -0.11127150803804398\n",
      "Message id:  2012726\n",
      "Cosine Sim Val : -0.10907010734081268\n",
      "Message id:  2059111\n",
      "Cosine Sim Val : -0.1054145097732544\n",
      "Message id:  2073893\n",
      "Cosine Sim Val : -0.10324677079916\n",
      "Message id:  1942946\n",
      "Cosine Sim Val : -0.10256258398294449\n",
      "Message id:  2078110\n",
      "Cosine Sim Val : -0.10097366571426392\n",
      "Message id:  2085340\n",
      "Cosine Sim Val : -0.1002906858921051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshara/.virtualenvs/augur_env/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Fetching message IDs with cosine similarity <= -0.09\n",
    "\n",
    "sorted_cosine_similarities = get_computed_similarities(vectors=doc2vec_vectors_test, predicted_vectors=predicted_vectors_test)\n",
    "unique_message_list = display_unique(sorted_cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique messages count: 19\n"
     ]
    }
   ],
   "source": [
    "# Estimated count of unique messages\n",
    "print('Unique messages count: '+str(len(unique_message_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of anomalous messages\n",
    "messages = df_present[df_present['msg_id'].isin(unique_message_list)]['msg_text'].tolist()\n",
    "message_dates = df_present[df_present['msg_id'].isin(unique_message_list)]['msg_timestamp'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Martin, \\r\\nI could retest my app, and STS does exactly, what I was looking for. I see my cxf path, my servlet produced by ServletRegistrationBean, and also added a Servlet which is registered by @ServletComponentScan - all three appear in Request Mapping view and in the path search.\\r\\nThanks again, looking forward to install the stable version!!!'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[12]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augur_env",
   "language": "python",
   "name": "augur_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
