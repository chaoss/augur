{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis on messages using modified SentiCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Among the various classifiers tried, XGBoost and GradientBoostingClassifier performed best with accuracies ~0.86\n",
    "#### Currently sentiment labels are being predicted: -1 -Negative, 0 -Neutral, 1 -Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from numpy import save, load\n",
    "import sqlalchemy as s\n",
    "import unicodedata\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Importing SentiCR\n",
    "from Sentiment import SentiCR\n",
    "# Perform training or use trained model if exists. \n",
    "sentiment_analyzer = SentiCR(algo = 'XGB' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to local Postgres database\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "database_connection_string = 'postgres+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = 25774\n",
    "\n",
    "# Fetch PR and issue messages of repo_id\n",
    "join_SQL = s.sql.text(\"\"\"\n",
    "       select message.msg_id, msg_timestamp,  msg_text from augur_data.message\n",
    "left outer join augur_data.pull_request_message_ref on message.msg_id = pull_request_message_ref.msg_id \n",
    "left outer join augur_data.pull_requests on pull_request_message_ref.pull_request_id = pull_requests.pull_request_id\n",
    "where repo_id = :repo_id\n",
    "UNION\n",
    "select message.msg_id, msg_timestamp, msg_text from augur_data.message\n",
    "left outer join augur_data.issue_message_ref on message.msg_id = issue_message_ref.msg_id \n",
    "left outer join augur_data.issues on issue_message_ref.issue_id = issues.issue_id\n",
    "where repo_id = :repo_id\n",
    "\"\"\")\n",
    "\n",
    "# Transfer to Pandas df\n",
    "df_message = pd.read_sql_query(join_SQL, engine, params={'repo_id': repo_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predicted sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentiment score\n",
    "def get_senti_score(df,col,label=False):\n",
    "    start_time = time.time()\n",
    "    siz = df.shape[0]\n",
    "    i = 0\n",
    "    labels = []\n",
    "    scores = []\n",
    "    while (i<siz):\n",
    "        if label:\n",
    "            x, y = sentiment_analyzer.get_sentiment_polarity(df.iloc[i][col],label)\n",
    "            labels.append(x)\n",
    "            scores.append(y)\n",
    "        else:\n",
    "            score = sentiment_analyzer.get_sentiment_polarity(df.iloc[i][col],label)\n",
    "            scores.append(score)\n",
    "        i+=1\n",
    "    scores = np.array(scores)\n",
    "    labels = np.array(labels)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    if label:\n",
    "        return (labels,scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting senti score on our data\n",
    "df_message['senti_label'], df_message['senti_score'] = get_senti_score(df_message,'msg_text',label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting scores on custom test set\n",
    "df_test = pd.read_csv('mod_test.csv')\n",
    "df_test['pred_senti_label'],df_test['pred_senti_score'] = get_senti_score(df_test,'Text',label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on custom test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results using XGB as classifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(df_test['score'], df_test['pred_senti_label'])\n",
    "print('Accuracy: '+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(df_test['score'], df_test['pred_senti_label'])\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('score').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(df_test['score'], df_test['pred_senti_label'])))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(df_test['score'], df_test['pred_senti_label'], average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(df_test['score'], df_test['pred_senti_label'], average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(df_test['score'], df_test['pred_senti_label'], average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(df_test['score'], df_test['pred_senti_label'], average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(df_test['score'], df_test['pred_senti_label'], average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(df_test['score'], df_test['pred_senti_label'], average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(df_test['score'], df_test['pred_senti_label'], average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(df_test['score'], df_test['pred_senti_label'], average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(df_test['score'], df_test['pred_senti_label'], average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(df_test['score'], df_test['pred_senti_label'], target_names=['Negative', 'Neutral', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_message.to_csv(f'senti_repo_{repo_id}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceed to timeseries analysis using sentiment trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_message.copy()\n",
    "df['date'] =  pd.to_datetime(df[\"msg_timestamp\"])\n",
    "df = df.drop(['msg_timestamp'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Grouping sentiments weekly for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='date')\n",
    "df1 = df.groupby(pd.Grouper(key='date', freq=\"w\"))['senti_label'].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.fillna(0)\n",
    "df1['total'] = df1.sum(axis=1)\n",
    "df1.columns = ['Negative','Neutral','Positive','Total']\n",
    "df1 = df1[df1['Positive']+df1['Negative']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.plot(df1.index, df1['Neutral'], color='blue', label = 'Neutral')\n",
    "ax.plot(df1.index, df1['Positive'], color='green', label = 'Positive')\n",
    "ax.plot(df1.index, df1['Negative'], color='orange', label = 'Negative') \n",
    "plt.title('Weekly Sentiment trend', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Anomaly detection based on trend\n",
    "\n",
    "#### Isolation forest applied to 2 features: Ratio of positive sentiment to the total and negative sentiment to total\n",
    "#### Ratio is being considered in order to take into account the total messages at any time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['PosR'] = df1['Positive']/df1['Total']\n",
    "df1['NegR'] = df1['Negative']/df1['Total']\n",
    "\n",
    "df1 = df1[df1['Positive']+df1['Negative']!=0]\n",
    "df1['PNRatio'] = df1['Positive']/df1['Negative']\n",
    "\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "features = ['PosR','NegR']\n",
    "clf = IsolationForest(n_estimators=100, max_samples='auto', max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)\n",
    "clf.fit(df1[features])\n",
    "\n",
    "pred = clf.predict(df1[features])\n",
    "df1['anomaly']=pred\n",
    "\n",
    "anomaly = df1.loc[df1['anomaly'] == -1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.plot(df1.index, df1['Total'], color='blue', label = 'Normal')\n",
    "ax.plot(df1.index, df1['Positive'], color='green', label = 'Positive')\n",
    "ax.plot(df1.index, df1['Negative'], color='orange', label = 'Negative') \n",
    "ax.scatter(anomaly.index,anomaly['Total'], color='red', label = 'Anomaly')\n",
    "plt.title('Anomaly in sentiment trend', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Next step is to calculate the overall sentiment score for every issue & PR\n",
    "\n",
    "#### Study the progress in the sentiment over the lifecycle of the issue and PR\n",
    "#### This will also be used as an additional feature for qualitatively analyzing the issues and PRs of every repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (augur)",
   "language": "python",
   "name": "augur"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
